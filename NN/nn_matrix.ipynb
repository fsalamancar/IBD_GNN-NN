{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pU4TOlZYHG0C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/bf/fdxk2h0901v934ldwnk_10x00000gn/T/ipykernel_88153/4039492962.py:9: DtypeWarning: Columns (1,2,3,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  protein_values_df = pd.read_csv(f'{ukb_data_path}/ibd0.tsv', sep='\\t')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ukb_data_path = '/Users/fjosesala/Library/CloudStorage/GoogleDrive-fsalamancar@unal.edu.co/Shared drives/UKB_Data/Raw_data'\n",
        "\n",
        "with open('/Users/fjosesala/Documents/GitHub/IBD_GNN-NN/NN/data/raw/proteins.txt', 'r') as txt_file:\n",
        "    columns = [line.strip() for line in txt_file.readlines()]\n",
        "\n",
        "protein_values_df = pd.read_csv(f'{ukb_data_path}/ibd0.tsv', sep='\\t')\n",
        "protein_values_df['class'] = np.where(protein_values_df['Disease'] == 'Control', 0, 1)\n",
        "weight_matrix_df = pd.read_csv('/Users/fjosesala/Documents/GitHub/IBD_GNN-NN/NN/data/raw/protein_scores.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gKYXtD6FQQRc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class\n",
            "0    18999\n",
            "1     3626\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Count people per class\n",
        "print(protein_values_df['class'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4oI9PE6MLirN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(22625, 1983)\n",
            "[[-0.1808   1.12915 -0.75435 ...  0.       0.       0.     ]\n",
            " [ 0.0956  -0.67875 -0.6113  ...  0.       0.       0.     ]\n",
            " [ 0.0952   0.01855 -0.66255 ...  0.       0.       0.     ]\n",
            " [ 0.       0.      -0.26505 ...  0.       0.       0.     ]\n",
            " [ 0.0414  -0.25105 -0.7799  ...  0.       0.       0.     ]]\n",
            "(1983, 1983)\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(22625, 1983)\n"
          ]
        }
      ],
      "source": [
        "protein_values_matrix = protein_values_df[columns].values\n",
        "# Replace nan with 0\n",
        "protein_values_matrix = np.nan_to_num(protein_values_matrix)\n",
        "weight_matrix = weight_matrix_df.values\n",
        "\n",
        "print(protein_values_matrix.shape)\n",
        "print(protein_values_matrix[:5])\n",
        "print(weight_matrix.shape)\n",
        "print(weight_matrix[:5])\n",
        "\n",
        "assert protein_values_matrix.shape[1] == weight_matrix.shape[0], \"Dimensions don't match for multiplication!\"\n",
        "\n",
        "# Multiply the matrices\n",
        "result_matrix = np.dot(protein_values_matrix, weight_matrix)\n",
        "print(result_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D0EKr2pZQXFI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.87458735  1.12915    -0.6959844  ...  0.          1.2309898\n",
            "   2.4537142 ]\n",
            " [ 0.431761   -0.67875    -0.41851385 ...  0.          0.390574\n",
            "  -0.362337  ]\n",
            " [ 1.23370855  0.01855    -0.8137977  ...  0.         -0.2483794\n",
            "  -1.6756953 ]\n",
            " [-1.1338386   0.         -0.26505    ...  0.         -0.1827758\n",
            "  -0.24352635]\n",
            " [-2.4618103  -0.25105    -0.99157145 ...  0.         -0.2804594\n",
            "  -1.3356045 ]]\n"
          ]
        }
      ],
      "source": [
        "# Print first rows of result matrix\n",
        "print(result_matrix[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8rf8GjENi-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing: lr=0.0001, neurons=256, batch_size=32, epochs=20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.0511 | Test Accuracy: 0.8391\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      1.00      0.91      3797\n",
            "           1       0.00      0.00      0.00       728\n",
            "\n",
            "    accuracy                           0.84      4525\n",
            "   macro avg       0.42      0.50      0.46      4525\n",
            "weighted avg       0.70      0.84      0.77      4525\n",
            "\n",
            "\n",
            "Testing: lr=0.0001, neurons=256, batch_size=32, epochs=40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "labels = protein_values_df['class'].values\n",
        "X = result_matrix\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Focal loss definition\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
        "        weight = alpha * y_true * tf.pow(1 - y_pred, gamma) + (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma)\n",
        "        return tf.reduce_mean(weight * cross_entropy)\n",
        "    return loss\n",
        "\n",
        "# Model builder function\n",
        "def create_model(learning_rate=0.0005, neurons=512):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(neurons, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=focal_loss(), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rates = [0.0001, 0.0005, 0.001]\n",
        "neurons_list = [256, 512, 1024]\n",
        "batch_sizes = [32, 64]\n",
        "epochs_list = [20, 40]\n",
        "threshold = 0.5\n",
        "\n",
        "# Grid search\n",
        "for lr in learning_rates:\n",
        "    for neurons in neurons_list:\n",
        "        for batch_size in batch_sizes:\n",
        "            for epochs in epochs_list:\n",
        "                print(f\"\\nTesting: lr={lr}, neurons={neurons}, batch_size={batch_size}, epochs={epochs}\")\n",
        "\n",
        "                # Apply BorderlineSMOTE\n",
        "                smote = BorderlineSMOTE(random_state=42)\n",
        "                X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "                # Build model\n",
        "                model = create_model(learning_rate=lr, neurons=neurons)\n",
        "\n",
        "                # Early stopping\n",
        "                early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "                # Train\n",
        "                history = model.fit(\n",
        "                    X_train_bal, y_train_bal,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                # Evaluate\n",
        "                loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "                print(f\"Test Loss: {loss:.4f} | Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "                # Predict with threshold tuning\n",
        "                y_pred_probs = model.predict(X_test)\n",
        "                y_pred = (y_pred_probs > threshold).astype(int)\n",
        "\n",
        "                # Report\n",
        "                print(classification_report(y_test, y_pred, zero_division=0))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
